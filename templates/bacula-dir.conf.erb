# DO NOT EDIT - Managed by Puppet
#
# Bacula Director Master Configuration
#   for <%= @director_server %>

# Define the name of this director so other clients can
# connect to it and work with our system
Director {
  Name = "<%= @director_server -%>:director"
  Query File = "/etc/bacula/scripts/query.sql"
  Working Directory = <%= @working_dir %>
  PID Directory = <%= @pid_dir %>
<% if @plugin_dir -%>
  Plugin Directory = <%= @plugin_dir %>
<% end -%>
  Maximum Concurrent Jobs = 5
  Password = "<%= @director_password -%>"
  Messages = "<%= @director_server -%>:messages:daemon"
<% if @use_tls -%>
  TLS Enable = yes
  TLS Require = <%= @tls_require ? 'yes':'no' %>
  TLS Verify Peer = <%= @tls_verify_peer ? 'yes':'no' %>
<% @tls_allowed_cn.each do |allowed_cn| -%>
  TLS Allowed CN = "<%= allowed_cn %>"
<% end -%>
<% if @tls_ca_cert -%>
  TLS CA Certificate File = <%= @tls_ca_cert %>
<% end -%>
<% if @tls_ca_cert_dir -%>
  TLS CA Certificate Dir = <%= @tls_ca_cert_dir %>
<% end -%>
  TLS Certificate = <%= @tls_cert %>
  TLS Key = <%= @tls_key %>
<% end -%>
}

# This is where the catalog information will be stored (basically
# this should be how to connect to whatever database we're using)
Catalog {
  Name = "<%= @director_server -%>:<%= @db_backend -%>"
  dbname = "<%= @db_database -%>"; dbdriver = <%= @db_driver %>
  <% if @db_backend != 'sqlite' -%>
  dbaddress = <%= @db_host %>; dbport = <%= @db_port %>; user = <%= @db_user %>; password = <%= @db_password %>
  <% end -%>
}

# Configure how the directory will log and/or send messages. This
# should should be for just about everything.
Messages {
  Name = "<%= @director_server -%>:messages:standard"
  Mail Command = "<%= @mail_command %>"
  Operator Command = "<%= @operator_command %>"
<% if @mail_to -%>
  Mail = "<%= @mail_to -%>" = all, !skipped
<% end -%>
<% if @mail_to_on_error -%>
  Mail On Error = "<%= @mail_to_on_error -%>" = all, !skipped
<% end -%>
  Operator = "<%= @mail_to_operator -%>" = mount
  Console = all, !skipped, !saved
  # WARNING! the following will create a file that you must cycle from
  #          time to time as it will grow indefinitely. However, it will
  #          also keep all your messages if they scroll off the console.
  Append = "/var/log/bacula/<%= @director_server -%>:director.log" = all, !skipped
  Catalog = all
}

# These are messages directly from the various daemons themselves.
Messages {
  Name = "<%= @director_server -%>:messages:daemon"
  Mail Command = "/usr/sbin/bsmtp -h localhost -f bacula@<%= scope.lookupvar('::fqdn') -%> -s \"Bacula Notice (from Director %d)\" %r"
  Mail = "<%= @mail_to_daemon -%>" = all, !skipped
  Console = all, !skipped, !saved
  Append = "/var/log/bacula/<%= @director_server -%>:director.log" = all, !skipped
}

# DEFAULT STORAGE SERVER ------------------------------------------------------
# All the clients will define their own Storage Daemon configuration as they
# will connect to a dedicated File device on that director (to aid Pool & Volume
# management along with concurrent access). This section will define a default
# Storage Daemon to connect to (using the standard FileStorage device) and a
# Pool which will be used with that as well.
Storage {
  Name = "<%= @storage_server -%>:storage:default"
  Address = <%= @storage_server %>
  Password = "<%= @director_password -%>"
  Device = "DefaultFileStorage"
  Media Type = File
<% if @use_tls -%>
  TLS Enable = yes
  TLS Require = <%= @tls_require ? 'yes':'no' %>
<% if @tls_ca_cert -%>
  TLS CA Certificate File = <%= @tls_ca_cert %>
<% end -%>
<% if @tls_ca_cert_dir -%>
  TLS CA Certificate Dir = <%= @tls_ca_cert_dir %>
<% end -%>
  TLS Certificate = <%= @tls_cert %>
  TLS Key = <%= @tls_key %>
<% end -%>
}

Pool {
  Name = "<%= @storage_server -%>:pool:default"
  Pool Type = Backup
  # Clean up any we don't need, and keep them for a maximum of a month (in
  # theory the same time period for weekly backups from the clients)
  # Note the files for the old volumes will still remain on the disk but will
  # be truncated to a zero size.
  Recycle = yes
  Auto Prune = <%= @volume_autoprune ? 'yes':'no' %>
  Action On Purge = Truncate
  Volume Retention = <%= @volume_retention %>
  Job Retention = 1 year
  Maximum Volume Bytes = <%= @volume_max_bytes %>   # Limit Volume size to something reasonable
  Maximum Volumes = <%= @volume_max_volumes %>      # Limit number of volumes in pool to avoid filling the disk
  Label Format = backupVolume
  Volume Use Duration = 24h;
  Recycle Oldest Volume = yes
}

Pool {
  Name = "<%= @storage_server -%>:pool:default.full"
  # All Volumes will have the format standard.date.time to ensure they
  # are kept unique throughout the operation and also aid quick analysis
  # We won't use a counter format for this at the moment.
  Label Format = "${Job}.full.${Year}${Month:p/2/0/r}${Day:p/2/0/r}.${Hour:p/2/0/r}${Minute:p/2/0/r}"
  Pool Type = Backup
  # Clean up any we don't need, and keep them for a maximum of a year
  # Note the files for the old volumes will still remain on the disk but will
  # be truncated to a zero size.
  Recycle = no
  Auto Prune = <%= @volume_autoprune_full ? 'yes':'no' %>
  Action On Purge = Truncate
  Volume Retention = <%= @volume_retention_full %>
  # Don't allow re-use of volumes; one volume per job only
  Maximum Volume Jobs = 1
}

Pool {
  Name = "<%= @storage_server_real -%>:pool:default.differential"
  # All Volumes will have the format standard.date.time to ensure they
  # are kept unique throughout the operation and also aid quick analysis
  # We won't use a counter format for this at the moment.
  Label Format = "${Job}.diff.${Year}${Month:p/2/0/r}${Day:p/2/0/r}.${Hour:p/2/0/r}${Minute:p/2/0/r}"
  Pool Type = Backup
  # Clean up any we don't need, and keep them for a maximum of fourty days
  # Note the files for the old volumes will still remain on the disk but will
  # be truncated to a zero size.
  Recycle = no
  Auto Prune = <%= @volume_autoprune_diff ? 'yes':'no' %>
  Action On Purge = Truncate
  Volume Retention = <%= @volume_retention_diff %>
  # Don't allow re-use of volumes; one volume per job only
  Maximum Volume Jobs = 1
}

Pool {
  Name = "<%= @storage_server -%>:pool:default.incremental"
  # All Volumes will have the format standard.date.time to ensure they
  # are kept unique throughout the operation and also aid quick analysis
  # We won't use a counter format for this at the moment.
  Label Format = "${Job}.incr.${Year}${Month:p/2/0/r}${Day:p/2/0/r}.${Hour:p/2/0/r}${Minute:p/2/0/r}"
  Pool Type = Backup
  # Clean up any we don't need, and keep them for a maximum of 40 days
  # Note the files for the old volumes will still remain on the disk but will
  # be truncated to a zero size.
  Recycle = no
  Auto Prune = <%= @volume_autoprune_incr ? 'yes':'no' %>
  Action On Purge = Truncate
  Volume Retention = <%= @volume_retention_incr %>
  # Don't allow re-use of volumes; one volume per job only
  Maximum Volume Jobs = 1
}

Pool {
  Name = "<%= @storage_server -%>:pool:catalog"
  # All Volumes will have the format director.catalog.date.time to ensure they
  # are kept unique throughout the operation and also aid quick analysis
  Label Format = "${Job}.<%= @director_server -%>.${Counter<%= @director_server.split('.')[0].capitalize.gsub(/[^[:alnum:]]/,'') -%>Catalog+:p/3/0/r}"
  Pool Type = Backup
  # Clean up any we don't need, and keep them for a maximum of a month (in
  # theory the same time period for weekly backups from the clients)
  Recycle = yes
  Auto Prune = yes
  # We have no limit on the number of volumes, but we will simply set that
  # we should keep at least one weeks worth of backups of the database
  Volume Retention = 1 Week
  # Don't allow re-use of volumes; one volume per job only
  Maximum Volume Jobs = 1
}

# Create a Counter which will be used to label the catalog volumes on the system
Counter {
  Name    = "Counter<%= @director_server.split('.')[0].capitalize.gsub(/[^[:alnum:]]/,'') -%>Catalog"
  Minimum = 1
  Catalog = "<%= @director_server -%>:<%= @db_backend %>"
}

# FILE SETS -------------------------------------------------------------------
# Define the standard set of locations which which will be backed up (along
# what within those should not be). In general, we have two types:
#
#   Basic:noHome     This doesn't back up the /home directory as its mounted
#                    from an NFS director on the network (this is the default).
#   Basic:withHome   This one does for servers where we don't mount NFS on it.

FileSet {
  Name = "Basic:noHome"
  Include {
    Options {
      Signature   = SHA1
      Compression = GZIP
    }

    # Don't worry about most of the director as Puppet manages the
    # configuration. Ensure that per-machine state files or settings
    # are backed up, along with stuff from /var or /srv which should be
    # most service-related files
    File = /boot
    File = /etc
    File = /usr/local
    File = /var
    File = /opt
    File = /srv
    # /home will not be backed up on any normal director as it's managed from
    # a central file-server for most servers.
  }

  Exclude {
    # Ignore stuff that can be ignored
    File = /var/cache
    File = /var/tmp
    # The state of the packages installed, or their files, etc.
    # can be ignored as we use puppet to rebuild much of the server
    File = /var/lib/apt
    File = /var/lib/dpkg
    File = /var/lib/puppet
    File = /var/lib/yum
    # Ignore database stuff; this will need to be handled
    # using some sort of a dump script
    File = /var/lib/mysql
    File = /var/lib/postgresql
    File = /var/lib/ldap
    # Bacula's state files are no use to us on restore
    File = <%= @var_dir %>
  }
}

FileSet {
  Name = "Basic:withHome"
  Include {
    Options {
      Signature   = SHA1
      Compression = GZIP
    }

    File = /boot
    File = /etc
    File = /usr/local
    File = /var
    File = /opt
    File = /srv
    # This set does include /home
    File = /home
  }

  Exclude {
    File = /var/cache
    File = /var/tmp
    File = /var/lib/apt
    File = /var/lib/dpkg
    File = /var/lib/puppet
    File = /var/lib/mysql
    File = /var/lib/postgresql
    File = /var/lib/ldap
    File = /var/lib/bacula
    File = /var/lib/yum
  }
}

# This set is specifically for Bacula to allow it to backup its own internal
# cataloge as part of the normal process.
FileSet {
  Name = "Catalog"
  Include {
    Options {
      Signature   = SHA1
      Compression = GZIP
    }
    File = "/var/spool/bacula/bacula.sql"
  }
}


# SCHEDULE --------------------------------------------------------------------
# Define when jobs should be run, and what Levels of backups they will be when
# they are run.

# These two are the default backup schedule; don't change them
Schedule {
  Name = "WeeklyCycle"
  Run = Level=Full First Sun at 23:05
  Run = Level=Differential Second-Fifth Sun at 23:05
  Run = Level=Incremental Mon-Sat at 23:05
}

# This does the catalog. It starts after the WeeklyCycle
Schedule {
  Name = "WeeklyCycleAfterBackup"
  Run = Level=Full Monday at 23:10
  Run = Level=Full Tuesday at 23:10
  Run = Level=Full Wednesday at 23:10
  Run = Level=Full Thursday at 23:10
  Run = Level=Full Friday at 23:10
}

# These cycles are set up so that we can spread out the full backups of our
# servers across the week. Some at the weekend, some mid-week.
Schedule {
  Name = "Weekly:onFriday"
  Run = Level=Full First Fri at 18:30
  Run = Level=Differential Second-Fifth Fri at 18:30
  Run = Level=Incremental Sat-Thu at 20:00
}

Schedule {
  Name = "Weekly:onSaturday"
  # Because this is a weekend job, we'll start the full runs earlier
  Run = Level=Full First Sat at 15:30
  Run = Level=Differential Second-Fifth Sat at 15:30
  Run = Level=Incremental Sun-Fri at 20:00
}

Schedule {
  Name = "Weekly:onSunday"
  # Because this is a weekend job, we'll start the full runs earlier
  Run = Level=Full First Sun at 15:30
  Run = Level=Differential Second-Fifth Sun at 15:30
  Run = Level=Incremental Mon-Sat at 20:00
}

Schedule {
  Name = "Weekly:onMonday"
  Run = Level=Full First Mon at 18:30
  Run = Level=Differential Second-Fifth Mon at 18:30
  Run = Level=Incremental Tue-Sun at 20:00
}

Schedule {
  Name = "Weekly:onTuesday"
  Run = Level=Full First Tue at 18:30
  Run = Level=Differential Second-Fifth Tue at 18:30
  Run = Level=Incremental Wed-Mon at 20:00
}

Schedule {
  Name = "Weekly:onWednesday"
  Run = Level=Full First Wed at 18:30
  Run = Level=Differential Second-Fifth Wed at 18:30
  Run = Level=Incremental Thu-Tue at 20:00
}

Schedule {
  Name = "Weekly:onThursday"
  Run = Level=Full First Thu at 18:30
  Run = Level=Differential Second-Fifth Thu at 18:30
  Run = Level=Incremental Fri-Wed at 20:00
}

Schedule {
  Name = "Hourly"
  Run = Level=Incremental hourly at 0:30
}

# JOB DEFINITIONS -------------------------------------------------------------
# Create the types of jobs we need to run.

<% if @backup_catalog -%>
# Backup the catalog database (after the nightly save)
Job {
  Name = "BackupCatalog"
  Type = Backup
  Client = <%= @director_server %>
  FileSet="Catalog"
  Schedule = "WeeklyCycleAfterBackup"
  Storage = "<%= @storage_server -%>:storage:default"
  Messages = "<%= @director_server -%>:messages:standard"
  Pool = "<%= @storage_server -%>:pool:catalog"
  # This creates an ASCII copy of the catalog
  RunBeforeJob = "/usr/libexec/bacula/make_catalog_backup.pl <%= @director_server -%>:<%= @db_backend -%>"
  # This deletes the copy of the catalog
  RunAfterJob  = "/usr/libexec/bacula/delete_catalog_backup"
  Write Bootstrap = "<%= @var_dir %>/BackupCatalog.bsr"
  # Run after main backup
  Priority = 50
# This doesn't seem to be working correctly, disable for now.
#  RunScript {
#   RunsWhen=After
#   RunsOnClient=no
#   Console = "purge volume action=all allpools storage=File"
#  }
}
<% end -%>

# Create a standard profile for all normal servers
<%
  ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'].each do |onDay|
    ['noHome','withHome'].each do |doHome| -%>
JobDefs {
  Name = "Basic:<%= doHome -%>:on<%= onDay -%>"
  Type = Backup
  Level = Incremental
  FileSet = "Basic:<%= doHome -%>"
  Schedule = "Weekly:on<%= onDay -%>"
  Messages = "<%= @director_server -%>:messages:standard"
  # Set the job to work as standard with the default Pool & Storage
  # (this will be overridden by the Job configuration for each Client)
  Storage = "<%= @storage_server -%>:storage:default"
  Pool = "<%= @storage_server -%>:pool:default"
  Write Bootstrap = "<%= @var_dir %>/%c.bsr"
  Priority = 15
  # Define how long any of these jobs are allowed to run for before we should
  # kill them. Note that this is the run time (how long the actual backup is
  # running for after starting, and not a maximum time after it was scheduled)
  Full Max Run Time = 36 Hours
  Differential Max Run Time = 6 Hours
  Incremental Max Run Time = 6 Hours
}
<%  end
  end -%>

# Finally, bring in all the additional pieces of configuration from the
# different servers for which this Director was configured to manage
@|"sh -c 'for f in /etc/bacula/bacula-dir.d/*.conf ; do echo @${f} ; done'"
